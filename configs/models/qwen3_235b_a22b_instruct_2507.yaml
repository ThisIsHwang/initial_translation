hf_model_id: Qwen/Qwen3-235B-A22B-Instruct-2507
served_model_name: qwen3-235b-a22b-instruct-2507
prompt_style: custom

vllm:
  port: 8000
  host: 0.0.0.0
  dtype: auto
  # For a single node with 8x H100, start with tensor parallel size 8.
  # If you see OOM, consider reducing max_model_len and/or using vLLM options like
  # FP8 KV cache / quantization via extra_args (hardware- and model-dependent).
  tensor_parallel_size: 8
  gpu_memory_utilization: 0.90
  max_model_len: 128000
  trust_remote_code: true
  extra_args: []

prompt:
  system: "You are a professional {source_lang} ({src_lang_code}) to {target_lang} ({tgt_lang_code}) translator. Your goal is to accurately convey the meaning and nuances of the original {source_lang} text while adhering to {target_lang} grammar, vocabulary, and cultural sensitivities.\nProduce only the {target_lang} translation, without any additional explanations or commentary."
  user: "Please translate the following {source_lang} text into {target_lang}:\n\n\n{text}"

generation_defaults:
  temperature: 0.0
  top_p: 1.0
  max_tokens: 64000
  stop: []
